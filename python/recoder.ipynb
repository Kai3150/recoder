{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1dc8206-16c6-4cd2-a420-bbc934ec6e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import datetime\n",
    "\n",
    "import MeCab\n",
    "\n",
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import URLError, HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a46e43a-9337-45c0-a1ca-0860093fa4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Gijiroku():\n",
    "    def __init__(self, clova_file, name, date):\n",
    "        self.date = date\n",
    "        self.name = name\n",
    "        self.paragraph_list = []\n",
    "        self.read_clova_txt(clova_file)\n",
    "        self.keywords = []\n",
    "        self.html = \"\"\n",
    "        \n",
    "    def read_clova_txt(self, clova_txt_path):  \n",
    "        speaker = \"\"\n",
    "        #バラバラな一人の発言をまとめる。\n",
    "        with open(clova_txt_path) as f:\n",
    "            lines = f.read()\n",
    "    \n",
    "        with open(clova_txt_path) as f:\n",
    "            for line in f:\n",
    "                line = line.rstrip()  # 読み込んだ行の末尾には改行文字があるので削除\n",
    "                if re.search(r'^参加者', line): #参加者line\n",
    "                    if speaker != ''.join(line.split(' ')[:2]):  #話者が変わるとき\n",
    "                        speaker = ''.join(line.split(' ')[:2])   #話者を更新\n",
    "                    else:\n",
    "                        lines = lines.replace(line,'')\n",
    "        with open(clova_txt_path, 'w') as f:\n",
    "            f.write(lines)\n",
    "  \n",
    "            \n",
    "        with open(clova_txt_path) as f:\n",
    "            #ヘッダーを消して参加者 1まで読み込む\n",
    "            while True:\n",
    "                last_point = f.tell()\n",
    "                line = f.readline()\n",
    "\n",
    "                if re.match(r'^参加者', line):\n",
    "                    f.seek(last_point)\n",
    "                    break\n",
    "\n",
    "            line = f.readline().rstrip()\n",
    "            speaker = '参加者 ' + line.split(' ')[1]   #話者を更新\n",
    "            time = line.split(' ')[2]                 #会話開始時間を記録\n",
    "            paragraph = Paragraph(speaker, time, '')\n",
    "            \n",
    "            for line in f:\n",
    "                line = line.rstrip()  # 読み込んだ行の末尾には改行文字があるので削除\n",
    "                if re.search(r'^参加者', line): #参加者line\n",
    "                    self.paragraph_list.append(paragraph)\n",
    "                    speaker = '参加者 ' + line.split(' ')[1]   #話者を更新\n",
    "                    time = line.split(' ')[2]                 #会話開始時間を記録\n",
    "                    paragraph = Paragraph(speaker, time, '')\n",
    "                else: #textline\n",
    "                    paragraph.text += line\n",
    "\n",
    "            self.paragraph_list.append(paragraph)\n",
    "        \n",
    "    def clenging(self):\n",
    "        delete_list = []\n",
    "        for i, paragraph in enumerate(self.paragraph_list):\n",
    "            paragraph.text_clenging()\n",
    "            if paragraph.text == \"\":\n",
    "                delete_list.append(i)\n",
    "                \n",
    "        for delete_key in sorted(delete_list, reverse=True):\n",
    "            self.paragraph_list.pop(delete_key)\n",
    "         \n",
    "    def show_all(self):\n",
    "        for paragraph in self.paragraph_list:\n",
    "            print(paragraph.speaker, paragraph.time)\n",
    "            print(paragraph.text)\n",
    "            \n",
    "    def delete_aizuti(self):\n",
    "        ##一番喋っている人を特定\n",
    "        max_n = 0\n",
    "        value_count = {}\n",
    "        for paragraph in self.paragraph_list:\n",
    "            if paragraph.speaker not in value_count.keys():\n",
    "                value_count[paragraph.speaker] = 0\n",
    "            else:\n",
    "                value_count[paragraph.speaker] += 1\n",
    "                if max_n < value_count[paragraph.speaker]:\n",
    "                    max_n = value_count[paragraph.speaker]\n",
    "                    speaker = paragraph.speaker\n",
    "\n",
    "        #ファシリテーターの相槌を消す。 上位3つだけ残す\n",
    "        fasili_dict = {}  #{リストのスライス: 文字数 }\n",
    "        for i, paragraph in enumerate(self.paragraph_list):\n",
    "            if paragraph.speaker == speaker: #ファシリテーターを特定\n",
    "                fasili_dict[i] = len(paragraph.text)\n",
    "\n",
    "        for i in range(4):\n",
    "            max_value = 0\n",
    "            for key, value in fasili_dict.items():\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    max_key = key\n",
    "            fasili_dict.pop(max_key)\n",
    "\n",
    "        for delete_key in sorted(fasili_dict.keys(), reverse=True):\n",
    "            self.paragraph_list.pop(delete_key)\n",
    "            \n",
    "    def text_merge(self):\n",
    "        merge_list = []\n",
    "        speaker = ''\n",
    "        for i, paragraph in enumerate(self.paragraph_list):\n",
    "            if speaker != paragraph.speaker:\n",
    "                speaker = paragraph.speaker\n",
    "            else:\n",
    "                merge_list.append(i)\n",
    "\n",
    "        for merge_index in sorted(merge_list, reverse=True):\n",
    "            self.paragraph_list[merge_index - 1].text = self.paragraph_list[merge_index - 1].text + self.paragraph_list[merge_index].text\n",
    "            self.paragraph_list.pop(merge_index)\n",
    "\n",
    "    def delete_less100(self):\n",
    "        delete_list = []\n",
    "        for i, paragraph in enumerate(self.paragraph_list):\n",
    "            paragraph.remove_less100()\n",
    "            if paragraph.text == \"\":\n",
    "                delete_list.append(i)\n",
    "                \n",
    "        for delete_key in sorted(delete_list, reverse=True):\n",
    "            self.paragraph_list.pop(delete_key)\n",
    "    \n",
    "    def summarize(self):\n",
    "        for paragraph in self.paragraph_list:\n",
    "            paragraph.summarize()\n",
    "            \n",
    "    def merge_wiki_words(self):\n",
    "        for paragraph in self.paragraph_list:\n",
    "            paragraph.get_wiki_words()\n",
    "            self.keywords.extend(paragraph.keywords)\n",
    "        self.keywords = set(self.keywords)\n",
    "        \n",
    "        #wikipediaに存在するか確認\n",
    "        checked_words = []\n",
    "        for word in self.keywords:\n",
    "            request_url = 'https://ja.wikipedia.org/api/rest_v1/page/summary/' + urllib.parse.quote(word)\n",
    "            req = Request(request_url)\n",
    "            try:\n",
    "                with urlopen(req) as res:\n",
    "                    res_json = res.read()\n",
    "            except HTTPError as e:\n",
    "                continue\n",
    "            except URLError as e:\n",
    "                continue\n",
    "            else:\n",
    "                wiki = json.loads(res_json.decode('utf-8'))\n",
    "                if (wiki['type'] == 'disambiguation') or (len(wiki['extract']) < 7):\n",
    "                    continue\n",
    "\n",
    "                checked_words.append(word)\n",
    "            time.sleep(0.01)\n",
    "        self.keywords = checked_words\n",
    "    \n",
    "    def render(self):\n",
    "        #リンクが貼れるように置換するための辞書を作成\n",
    "        checked_dict = {}\n",
    "        for word in self.keywords:\n",
    "            checked_dict[word] = \"<span class=\\\"wiki\\\">\" + word + \"</span>\"\n",
    "    \n",
    "        for paragraph in self.paragraph_list:\n",
    "            paragraph.render()\n",
    "        self.clenging()\n",
    "        \n",
    "        body=''\n",
    "        for paragraph in self.paragraph_list:\n",
    "            body = body + '{html}'.format(html=paragraph.html)\n",
    "\n",
    "        #辞書をもとにhtml用に書き換え\n",
    "        for word, read in checked_dict.items():\n",
    "            body = body.replace(word, read)\n",
    "\n",
    "        dt_now = datetime.datetime.now()\n",
    "        date=dt_now.strftime('%Y年%m月%d日') + \"の議事録\"\n",
    "\n",
    "        self.html = '''<!DOCTYPE html>\n",
    "        <html lang=\"ja\">\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <title>議事録</title>\n",
    "            <link rel=\"stylesheet\" href=\"static/style.css\">\n",
    "            <link rel=\"stylesheet\" href=\"static/nobata.css\">\n",
    "        </head>\n",
    "            <body>\n",
    "                <h1>{date}</h1>\n",
    "                <h1>画像アップロード</h1>\n",
    "                <form action=\"/upload\" method=\"post\" enctype=\"multipart/form-data\" class=\"form-img\">\n",
    "                    <div id=\"drop-zone\" style=\"border: 1px solid; padding: 30px; border-color: white;\">\n",
    "                        <p>ファイルをドラッグ＆ドロップもしくは</p>\n",
    "                        <input type=\"file\" name=\"file\" id=\"file-input\">\n",
    "                    </div>\n",
    "                    <h2>プレビュー</h2>\n",
    "                    <div id=\"preview\"></div>\n",
    "                    <h2>アップロードした画像</h2>\n",
    "                    <div id=\"uploaded\"></div>\n",
    "                    <input type=\"submit\" style=\"margin-top: 50px\">\n",
    "                </form>\n",
    "                <audio controls src=\"audio/output.mp3\" id=\"audio\"></audio>\n",
    "                {body}\n",
    "                <script src=\"https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js\"></script>\n",
    "                <script src=\"static/wikipedia-preview.development.js\"></script>\n",
    "                <script src=\"static/file.js\"></script>\n",
    "                <script type=\"text/javascript\">wikipediaPreview.init({{lang: 'ja'}});</script>\n",
    "            </body>\n",
    "        </html>'''.format(date=date, body=body) \n",
    "\n",
    "        with open(\"../output/public/gijiroku/gijiroku\"+ datetime.datetime.now().strftime('(%Y.%m.%d)') + '.html', 'w', encoding='utf-8') as f: \n",
    "            f.write(self.html) \n",
    "\n",
    "# Taggerオブジェクトを生成\n",
    "tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
    "tokenizer.parse(\"\")\n",
    "\n",
    "class Paragraph:\n",
    "    def __init__(self, speaker, time, text):\n",
    "        self.speaker = speaker\n",
    "        self.time = time\n",
    "        self.text = text\n",
    "        self.keywords = []\n",
    "        self.html = \"\"\n",
    "        \n",
    "    def text_clenging(self):\n",
    "        self.text = re.sub(' ', '、', self.text) ##空白削除\n",
    "\n",
    "        self.text = self.text.replace('です', 'です。').replace('ます','ます。').replace('でした','でした。').replace('ません','ません。').replace('さい','さい。')##ますの後には必ず「。」\n",
    "        self.text = self.text.replace('っていうこと', 'こと').replace('っていう', 'という').replace('ていう', 'という').replace('かなと', 'かと')##ますの後には必ず「。」\n",
    "\n",
    "        self.text = re.sub(r'(えー|えーと|えっと|そうですね|まあ|じゃあ|なんか|ちょっと|あの|ということで|っていうの|んじゃないか|一応|とりあえず)', '', self.text) ##削除\n",
    "        self.text = re.sub(r'ま([^\\u3040-\\u309F])', r'\\1', self.text) ##削除\n",
    "        self.text = re.sub(r'という([、。])', r'\\1', self.text) ##削除\n",
    "        self.text = re.sub(r'(.)(.)(.)\\1\\2\\3', r'\\1\\2\\3', self.text)#繰り返し文字\n",
    "        self.text = re.sub(r'([\\u3400-\\u9FFF\\uF900-\\uFAFF]|[\\uD840-\\uD87F][\\uDC00-\\uDFFF])([\\u3400-\\u9FFF\\uF900-\\uFAFF]|[\\uD840-\\uD87F][\\uDC00-\\uDFFF])\\1\\2', r'\\1\\2', self.text)\n",
    "\n",
    "        while re.search(r'([ねえま][、。]|[、。].{0,2}[、。])', self.text):\n",
    "            self.text = re.sub(r'([ねえま]、)', '、', self.text) ##誤字 語感　訂正\n",
    "            self.text = re.sub(r'([ねえま]。)', '。', self.text) ##誤字　語感　訂正\n",
    "\n",
    "            self.text = re.sub(r'[、].{0,2}[、。]', '、', self.text) ##削除\n",
    "            self.text = re.sub(r'[。].{0,2}[、。]', '。', self.text) ##削除\n",
    "            self.text = re.sub(r'^.{0,2}[、。]', '', self.text) ##削除\n",
    "            \n",
    "    def remove_less100(self):\n",
    "        if len(self.text) <= 100:\n",
    "            self.text =  \"\"\n",
    "    \n",
    "    def get_wiki_words(self):\n",
    "        #Wikipediaリンク用の単語を抽出\n",
    "        words = []\n",
    "        # 単語の特徴リストを生成\n",
    "        node = tokenizer.parseToNode(self.text)\n",
    "        while node:\n",
    "            # 品詞情報(node.feature)が名詞ならば\n",
    "            if node.feature.split(\",\")[0] == u\"名詞\":\n",
    "                #ひらがな、漢字、2文字以下のカタカナをパス\n",
    "                if (re.fullmatch(r'[\\u3040-\\u309F]+|[0-9]+|.', node.surface) == None)\\\n",
    "                and (re.fullmatch(r'[ァ-ヶ]{0,2}', node.surface) == None)\\\n",
    "                and (re.fullmatch(r'[\\u3040-\\u309F\\u2E80-\\u2FDF\\u3005-\\u3007\\u3400-\\u4DBF\\u4E00-\\u9FFF\\uF900-\\uFAFF\\U00020000-\\U0002EBEF]+', node.surface) == None)\\\n",
    "                and (re.match(r'(宮崎ゼミ|宮崎|ゼミ)',node.surface) == None): \n",
    "                    # 単語(node.surface)をwordsに追加\n",
    "                    words.append(node.surface)\n",
    "            node = node.next\n",
    "        #重複を削除\n",
    "        words = set(words)\n",
    "        self.keywords = list(words)\n",
    "    \n",
    "    def summarize(self):\n",
    "        count = len(self.text.split('。'))\n",
    "        count = round(count/4)#25%のこし\n",
    "\n",
    "        LANGUAGE = \"japanese\"  # 言語指定\n",
    "        SENTENCES_COUNT = count  # 要約文数\n",
    "\n",
    "\n",
    "        # parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "        parser = PlaintextParser.from_string(self.text, Tokenizer(LANGUAGE))\n",
    "        stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "        summarizer = Summarizer(stemmer)\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "        sentences = \"\"\n",
    "        for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "            sentences = sentences + sentence.__str__()\n",
    "            \n",
    "        self.text = sentences\n",
    "\n",
    "    def render(self):\n",
    "        times = self.time.split(':')\n",
    "        if len(times) == 2:\n",
    "            sec = int(times[0]) * 60 + int(times[1])\n",
    "        elif len(times) == 3:\n",
    "            sec = int(times[0]) * 360 + int(times[1]) * 60 + int(times[2])\n",
    "        #長すぎる文章は印\n",
    "        confirm_value = \"\"\n",
    "        if len(self.text) > 500:\n",
    "            confirm_value = \"yes\"\n",
    "\n",
    "        key_word = ' '.join(self.keywords[:round(len(self.keywords)/3)])\n",
    "\n",
    "        if confirm_value == '':\n",
    "            div = '''\n",
    "            <div class=\"confirm_value\" time=\"{time}\">\n",
    "                <p>{value}</p>\n",
    "            </div>'''.format(value=self.text, time=sec)\n",
    "        else:\n",
    "            div = '''\n",
    "            <div class=\"confirm_value long\" time=\"{time}\">\n",
    "                <p>{value}</p>\n",
    "            </div>'''.format(value=self.text, time=sec)\n",
    "\n",
    "        self.html = '''\n",
    "        <h2>{speaker}</h2>\n",
    "        <h3>{key_word}</h3>{div}'''.format(speaker=self.speaker, key_word=key_word, div=div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac64afc-5642-45a9-b7c7-cecbae952996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8834f347-a2d1-4fb9-bdfc-c36d0ac209fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gijiroku = Gijiroku('output.txt', '宮崎ゼミ', '10/28')\n",
    "gijiroku.clenging()\n",
    "gijiroku.delete_aizuti()\n",
    "gijiroku.delete_aizuti()\n",
    "gijiroku.text_merge()\n",
    "gijiroku.delete_less100()\n",
    "gijiroku.text_merge()\n",
    "gijiroku.clenging()\n",
    "gijiroku.summarize()\n",
    "gijiroku.delete_less100()\n",
    "gijiroku.text_merge()\n",
    "gijiroku.merge_wiki_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fa66d55-4fdb-4687-9c41-d33d47eec79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gijiroku.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
