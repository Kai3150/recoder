{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1dc8206-16c6-4cd2-a420-bbc934ec6e1e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/150nr66x0rvc2pp5m1_w6py80000gn/T/ipykernel_77574/3741020829.py:26: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import pkg_resources, imp\n",
      "/Users/nobatakai/Documents/gijiroku/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# scikit-learnのTF-IDFライブラリをインポート\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import MeCab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pkg_resources, imp\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import gensim\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "from ginza import *\n",
    "import spacy_transformers\n",
    "import time\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import urlencode\n",
    "from urllib.error import URLError, HTTPError\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab16c9d-089b-41db-a536-d9ba57ba2320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_clova_txt(clova_txt_path):  \n",
    "    text_dict = {}\n",
    "    speaker = \"\"\n",
    "    speaker_counted = \"\" #辞書のキー\n",
    "    speak_counter_dict = {}\n",
    "\n",
    "    with open(clova_txt_path) as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            line = line.rstrip()  # 読み込んだ行の末尾には改行文字があるので削除\n",
    "            if re.search(r'^参加者', line):\n",
    "                if speaker != line:  #話者が変わるとき\n",
    "                    speaker = line   #話者を更新\n",
    "\n",
    "                    #if speaker_counted not in text_dict.keys():\n",
    "                    if speaker not in speak_counter_dict.keys(): #初めて喋る人\n",
    "                        speak_counter_dict[speaker] = 0\n",
    "                        speaker_counted = speaker + \" \" + str(speak_counter_dict[speaker])\n",
    "                        text_dict[speaker_counted] = \"\"  # 喋る内容の準備\n",
    "\n",
    "                    else: #前にも話していた時\n",
    "                        speak_counter_dict[speaker] = speak_counter_dict[speaker] + 1 #会話数をプラス１\n",
    "                        speaker_counted = speaker + \" \" + str(speak_counter_dict[speaker])\n",
    "                        text_dict[speaker_counted] = \"\"\n",
    "\n",
    "            else:\n",
    "                text_dict[speaker_counted] += line\n",
    "    return text_dict\n",
    "\n",
    "def read_sloos_csv(sloos_csv_path):\n",
    "\n",
    "    df = pd.read_csv(sloos_csv_path)\n",
    "\n",
    "    text_dict = {}\n",
    "    speaker = \"\"\n",
    "    speaker_counted = \"\" #辞書のキー\n",
    "    speak_counter_dict = {}\n",
    "\n",
    "\n",
    "    for row in df.iterrows():\n",
    "\n",
    "        if speaker != row[1]['speaker']:  #話者が変わるとき\n",
    "            speaker = row[1]['speaker']   #話者を更新\n",
    "\n",
    "            #if speaker_counted not in text_dict.keys():\n",
    "            if speaker not in speak_counter_dict.keys(): #初めて喋る人\n",
    "                speak_counter_dict[speaker] = 0\n",
    "                speaker_counted = str(speaker) + str(speak_counter_dict[speaker])\n",
    "                text_dict[speaker_counted] = \"\"  # 喋る内容の準備\n",
    "\n",
    "            else: #前にも話していた時\n",
    "                speak_counter_dict[speaker] = speak_counter_dict[speaker] + 1 #会話数をプラス１\n",
    "                speaker_counted = str(speaker) + str(speak_counter_dict[speaker])\n",
    "                text_dict[speaker_counted] = \"\"\n",
    "\n",
    "        text_dict[speaker_counted] += row[1]['message'] + '。'\n",
    "\n",
    "    return text_dict\n",
    "\n",
    "def summarize(text: str, count=10) -> str:\n",
    "    LANGUAGE = \"japanese\"  # 言語指定\n",
    "    SENTENCES_COUNT = count  # 要約文数\n",
    "\n",
    "\n",
    "    # parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    sentences = \"\"\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sentences = sentences + sentence.__str__()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub(' ', '', text) ##空白削除\n",
    "   \n",
    "    text = text.replace('です', 'です。').replace('ます','ます。').replace('でした','でした。').replace('ません','ません。')##ますの後には必ず「。」\n",
    "    text = re.sub(r'[、。](ただ|でも|いや|だが|しかし|対して|一方で|あるいは|けれども|けども|けど|が)[、。]', '#BUT', text) ##逆説->#BUT\n",
    "    text = re.sub(r'[、。](たとえば|例えば)[、。]', '#EXAMPLE', text) ##具体例->#EXAMPLE\n",
    "    text = re.sub(r'[、。](だって)[、。]', '#BECAUSE_REASON', text) ##理由1->#BECAUSE_REASON\n",
    "    text = re.sub(r'[、。](だから|なので|従って)[、。]', '#REASON_BECAUSE', text) ##理由2->#REASON_BECAUSE\n",
    "    text = re.sub(r'[、。](つまり|まとめると|ですから)[、。]', '#INANUT', text) ##まとめor重要->#INANUT\n",
    "    text = re.sub(r'[、。](加えて|それと|そして|それから)[、。]', '#AND', text) ##加えて->#AND\n",
    "\n",
    "\n",
    "    text = re.sub(r'[、].{0,5}[、。]', '、', text) ##削除\n",
    "    text = re.sub(r'[。].{0,5}[、。]', '。', text) ##削除\n",
    "    text = re.sub(r'(えー)', '', text) ##削除\n",
    "\n",
    "    text = re.sub(r'(え[、。])', '、', text) ##誤字訂正\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'ます、', 'ます。', text) ##ますの後には必ず「。」\n",
    "    #re.findall(r'[、。].{6}[、。]', text)\n",
    "    \n",
    "    #print(text)\n",
    "    return text\n",
    "\n",
    "def write1(file, str1): \n",
    "    with open(file, 'w', encoding='utf-8') as f1: \n",
    "        f1.write(str1) \n",
    "\n",
    "        \n",
    "def convert(text):\n",
    "    # ノイズ削除\n",
    "    text = re.sub(r'《.+?》', '', text)\n",
    "    text = re.sub(r'［＃.+?］', '', text)\n",
    "    text = re.sub(r'｜', '', text)\n",
    "    text = re.sub(r'\\r\\n', '', text)\n",
    "    text = re.sub(r'\\u3000', '', text)  \n",
    "    text = re.sub(r'「', '', text) \n",
    "    text = re.sub(r'」', '', text)\n",
    "    text = re.sub(r'、', '', text)\n",
    "    text = re.sub(r'。', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Taggerオブジェクトを生成\n",
    "tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
    "tokenizer.parse(\"\")\n",
    "\n",
    "def extract(text):\n",
    "    words = []\n",
    "\n",
    "    # 単語の特徴リストを生成\n",
    "    node = tokenizer.parseToNode(text)\n",
    "\n",
    "    while node:\n",
    "        # 品詞情報(node.feature)が名詞ならば\n",
    "        if node.feature.split(\",\")[0] == u\"名詞\":\n",
    "            if re.fullmatch(r'[\\u3040-\\u309F]+', node.surface)== None: #ひらがなだけはパス\n",
    "                # 単語(node.surface)をwordsに追加\n",
    "                words.append(node.surface)\n",
    "        node = node.next\n",
    "\n",
    "    # 半角スペース区切りで文字列を結合\n",
    "    text_result = ' '.join(words)\n",
    "    return text_result\n",
    "\n",
    "def severe_extract(text):\n",
    "    words = []\n",
    "\n",
    "    # 単語の特徴リストを生成\n",
    "    node = tokenizer.parseToNode(text)\n",
    "\n",
    "    while node:\n",
    "        # 品詞情報(node.feature)が名詞ならば\n",
    "        if node.feature.split(\",\")[0] == u\"名詞\":\n",
    "            #ひらがな、2文字以下のカタカナ　を　パス\n",
    "            if (re.fullmatch(r'[\\u3040-\\u309F]+|[0-9]+|.', node.surface) == None)\\\n",
    "            and (re.fullmatch(r'[ァ-ヶ]{0,2}', node.surface) == None): \n",
    "                # 単語(node.surface)をwordsに追加\n",
    "                words.append(node.surface)\n",
    "        node = node.next\n",
    "\n",
    "    return words\n",
    "\n",
    "#TfidVectorizer(全体的で見た時の特徴語抽出)  どうでもいい言葉が多い感じ. 両方使ってもいいかもしれない\n",
    "def key_words(p_text_dict: dict) -> dict:\n",
    "    docs = []\n",
    "    for key, value in p_text_dict.items():\n",
    "        text = convert(value)\n",
    "        text = extract(text)\n",
    "        docs.append(text)\n",
    "\n",
    "    # モデルを生成\n",
    "    vectorizer = TfidfVectorizer(smooth_idf=False)\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # データフレームに表現\n",
    "    values = X.toarray()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    df = pd.DataFrame(values, columns = feature_names, index=p_text_dict.keys())\n",
    "\n",
    "    top10_dict = {}\n",
    "    for key in p_text_dict.keys():\n",
    "        top10_dict[key] = df.T[key].sort_values(ascending=False).head(10).keys()\n",
    "    return top10_dict\n",
    "\n",
    "\n",
    "#文章を文脈で分割\n",
    "\n",
    "#わかち書き関数\n",
    "def wakachi(text):\n",
    "    from janome.tokenizer import Tokenizer\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(text)\n",
    "    docs=[]\n",
    "    for token in tokens:\n",
    "        docs.append(token.surface)\n",
    "    return docs\n",
    " \n",
    "#文書ベクトル化関数\n",
    "def vecs_array(documents):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "    docs = np.array(documents)\n",
    "    vectorizer = TfidfVectorizer(analyzer=wakachi,binary=True,use_idf=False)\n",
    "    vecs = vectorizer.fit_transform(docs)\n",
    "    return vecs.toarray()\n",
    "\n",
    "def divide_text(text: str) -> list:\n",
    "    #三文ごとに分割\n",
    "    sample_slice = re.findall(\"[^。]+。?\", text)\n",
    "\n",
    "    i = 0\n",
    "    new_slice = []\n",
    "    temp_slice = []\n",
    "    for sentence in sample_slice:\n",
    "        temp_slice.append(sentence)\n",
    "\n",
    "        if i % 3 == 2:\n",
    "            new_slice.append(''.join(temp_slice))\n",
    "            temp_slice = []\n",
    "        i += 1\n",
    "    \n",
    "    docs = new_slice\n",
    "\n",
    "    #類似度行列作成\n",
    "    cs_array = np.round(cosine_similarity(vecs_array(docs), vecs_array(docs)),3)\n",
    "\n",
    "    xy=np.where(cs_array < 0.3)\n",
    "\n",
    "    x = xy[0][abs(xy[0] - xy[1]) == 1]\n",
    "    y = xy[1][abs(xy[0] - xy[1]) == 1]\n",
    "\n",
    "    diffs = pd.DataFrame([x, y])\n",
    "    t = diffs.T\n",
    "    t = t[t[0] > t[1]]\n",
    "    \n",
    "    #<h3>{key_words}</h3>\n",
    "    #<p>{content}</p>\n",
    "\n",
    "    #content = \"\"\" <h4>{speaker}</h4>  \n",
    "    #               <p>{naiyou}</p>           \"\"\"\n",
    "\n",
    "    #context_dict { key_words: contents}\n",
    "\n",
    "    for _, item in t.iterrows():\n",
    "        new_slice[item[0]] += '##DIV##'  \n",
    "\n",
    "    new_text = ''.join(new_slice)\n",
    "    divided_list = new_text.split('##DIV##')\n",
    "    \n",
    "    #add_key_words(new_slice)\n",
    "    #return context_dict\n",
    "    return divided_list\n",
    "def divide_text2(text: str) -> list:\n",
    "    slice_list = re.findall(\"([^。]+。)\", text)\n",
    "    while len(slice_list) > 30:\n",
    "        max_ratio = 0\n",
    "        divide_list = []\n",
    "        sames = []\n",
    "\n",
    "        for i in range(len(slice_list)-1):\n",
    "\n",
    "            str1 = slice_list[i]\n",
    "            str2 = slice_list[i+1]\n",
    "            \n",
    "\n",
    "            s = SequenceMatcher(None, str1, str2)\n",
    "\n",
    "            if s.ratio() > max_ratio:\n",
    "                max_ratio = s.ratio()\n",
    "                max_i = i\n",
    "                divide_list = [str1, str2]\n",
    "\n",
    "        slice_list[max_i] = slice_list[max_i] + slice_list.pop(max_i + 1)\n",
    "    return slice_list\n",
    "\n",
    "\n",
    "def add_key_words(new_slice):\n",
    "    context_list = []\n",
    "    key_word = 0\n",
    "    contents = ''\n",
    "    context_dict = {}\n",
    "    \n",
    "\n",
    "    for sentence3 in new_slice:\n",
    "        if '##DIV##' in sentence3:\n",
    "            sentence3 = sentence3.replace('##DIV##', '')\n",
    "            contents = ''\n",
    "            contents += sentence3\n",
    "\n",
    "            ##キーワードの抽出するならここ\n",
    "            key_word = key_word + 1\n",
    "\n",
    "            #context_list.append(contents)\n",
    "            context_dict[key_word] = contents\n",
    "        else:\n",
    "            contents += sentence3\n",
    "    return context_dict\n",
    "\n",
    "\n",
    "imp.reload(pkg_resources)\n",
    "# モデルのロード\n",
    "nlp = spacy.load(\"ja_core_news_md\")\n",
    "#nlp = spacy.load('ja_ginza')\n",
    "\n",
    "def get_key_word2(input_sentence:str) -> str:\n",
    "    # 解析対象のテキストa\n",
    "    input_text = input_sentence\n",
    "\n",
    "    input_text = re.sub(r'[#A-Z_]+', '', input_text)\n",
    "    # モデルに解析対象のテキストを渡す\n",
    "    doc = nlp(input_text)\n",
    "    # 固有表現を抽出\n",
    "    #for ent in doc.ents: \n",
    "        #print(ent.text, ent.label_, ent.start_char, ent.end_char)\n",
    "\n",
    "    # out --> \n",
    "    ## 2018年 DATE 0 5\n",
    "    ## 8月 DATE 6 8\n",
    "    ## フランス GPE 12 16\n",
    "    ## ルーヴル美術館 ORG 24 31\n",
    "    key_words = []\n",
    "    #for tok in doc:\n",
    "     #モデルが ja_ginza の場合\n",
    "        #if tok.pos_ == 'PROPN':\n",
    "            #key_words.append(tok.text)\n",
    "    for ent in doc.ents: \n",
    "        #モデルが ja_core_news_md の場合\n",
    "        if ent.label_ in ['ORG', 'PERSON', 'PRODUCT', 'GPE']:\n",
    "            key_words.append(ent.text)\n",
    "    key_words = list(set(key_words))#重複要素を削除\n",
    "    key_str = '、'.join(key_words)\n",
    "    return key_str\n",
    "\n",
    "\n",
    "def clean_text_api(text):\n",
    "    API=\"https://api.a3rt.recruit.co.jp/proofreading/v2/typo\"\n",
    "    KEY=\"DZZps9cHfJGAxGlvhqkYh0xHlIk8igKu\"\n",
    " \n",
    "    cleaned_text = ''\n",
    "    text_list = re.findall(\"[^。]+。?\", text)\n",
    "    for sentence in text_list:\n",
    "        quoted_text = sentence\n",
    "        values = {\n",
    "        'apikey': KEY,\n",
    "        'sentence':quoted_text,\n",
    "        'sensitivity':\"low\",\n",
    "        }\n",
    "\n",
    "        # パラメータをURLエンコードする\n",
    "        params = urllib.parse.urlencode(values)\n",
    "        # リクエスト用のURLを生成\n",
    "        url = API + \"?\" + params\n",
    "\n",
    "        #リクエストを投げて結果を取得\n",
    "        r = requests.get(url)\n",
    "        #辞書型に変換\n",
    "        data = json.loads(r.text)\n",
    "\n",
    "        if 'alerts' in data:\n",
    "\n",
    "            for alert in data['alerts']:\n",
    "                miss_word = alert['word']\n",
    "                suggested_word = alert['suggestions'][0]\n",
    "                quoted_text = quoted_text.replace(miss_word, suggested_word)\n",
    "        cleaned_text = cleaned_text + quoted_text\n",
    "    return cleaned_text\n",
    "def text_clenging(text:str) -> str:\n",
    "    \n",
    "    text = re.sub(' ', '、', text) ##空白削除\n",
    "   \n",
    "    text = text.replace('です', 'です。').replace('ます','ます。').replace('でした','でした。').replace('ません','ません。').replace('さい','さい。')##ますの後には必ず「。」\n",
    "    text = text.replace('っていうこと', 'こと').replace('っていう', 'という').replace('ていう', 'という').replace('かなと', 'かと')##ますの後には必ず「。」\n",
    "    \n",
    "    text = re.sub(r'(えー|えーと|えっと|そうですね|まあ|じゃあ|なんか|ちょっと|あの|ということで|っていうの|んじゃないか|一応|とりあえず)', '', text) ##削除\n",
    "    text = re.sub(r'ま([^\\u3040-\\u309F])', r'\\1', text) ##削除\n",
    "    text = re.sub(r'という([、。])', r'\\1', text) ##削除\n",
    "    text = re.sub(r'(.)(.)(.)\\1\\2\\3', r'\\1\\2\\3', text)#繰り返し文字\n",
    "    text = re.sub(r'([\\u3400-\\u9FFF\\uF900-\\uFAFF]|[\\uD840-\\uD87F][\\uDC00-\\uDFFF])([\\u3400-\\u9FFF\\uF900-\\uFAFF]|[\\uD840-\\uD87F][\\uDC00-\\uDFFF])\\1\\2', r'\\1\\2', text)\n",
    "    \n",
    "    while re.search(r'([ねえま][、。]|[、。].{0,2}[、。])', text):\n",
    "        text = re.sub(r'([ねえま]、)', '、', text) ##誤字 語感　訂正\n",
    "        text = re.sub(r'([ねえま]。)', '。', text) ##誤字　語感　訂正\n",
    "        \n",
    "        text = re.sub(r'[、].{0,2}[、。]', '、', text) ##削除\n",
    "        text = re.sub(r'[。].{0,2}[、。]', '。', text) ##削除\n",
    "        text = re.sub(r'^.{0,2}[、。]', '', text) ##削除\n",
    "        \n",
    "\n",
    "    return text\n",
    "\n",
    "def keyword_clenging(key_str: str)->str:\n",
    "    key_str = re.sub(r'(宮崎ゼミ|宮崎|ゼミ)', '', key_str)\n",
    "    key_str = re.sub(r'(、、)', '、', key_str)\n",
    "    key_str = re.sub(r'^、', '', key_str)\n",
    "    return key_str\n",
    "\n",
    "#s_text_dict -> よく喋る人を3つ残した同じdict\n",
    "def delete_aizuti(s_text_dict: dict) -> dict:\n",
    "    ##一番喋っている人を特定\n",
    "    max_n = 0\n",
    "    for key in s_text_dict.keys():\n",
    "        if max_n < int(key.split(' ')[2]): #'参加者 ID N' をスプリット\n",
    "            max_n = int(key.split(' ')[2])\n",
    "            speaker_index = key.split()[1]\n",
    "\n",
    "    fasili_dict = {}\n",
    "    #ファシリテーターの相槌を消す。 上位3つだけ残す\n",
    "    for key, value in s_text_dict.items():\n",
    "        if key.split(' ')[1] == speaker_index:\n",
    "            #ファシリテーターを特定\n",
    "            fasili_dict[key] = value\n",
    "\n",
    "    top3_key = []\n",
    "    while len(top3_key) < 3:\n",
    "        max_value = 0\n",
    "        for key, value in fasili_dict.items():\n",
    "            if len(value) > max_value:\n",
    "                max_value = len(value)\n",
    "                max_key = key\n",
    "        fasili_dict.pop(max_key)\n",
    "        top3_key.append(max_key)\n",
    "    for delete_key in list(fasili_dict.keys()):\n",
    "        s_text_dict.pop(delete_key)\n",
    "\n",
    "    return s_text_dict\n",
    "\n",
    "#話者が同じやつはくっつける\n",
    "def text_merge(p_text_dict):\n",
    "    speaker = \"\"\n",
    "    speaker_counted = \"\" #辞書のキー\n",
    "    speak_counter_dict = {}\n",
    "    s_text_dict = {}\n",
    "    for key, value in p_text_dict.items():\n",
    "        if speaker != key.split(' ')[1]:  #話者が変わるとき\n",
    "            speaker = key.split(' ')[1]   #話者を更新\n",
    "\n",
    "            #if speaker_counted not in text_dict.keys():\n",
    "            if speaker not in speak_counter_dict.keys(): #初めて喋る人\n",
    "                speak_counter_dict[speaker] = 0\n",
    "                speaker_counted = speaker + \" \" + str(speak_counter_dict[speaker])\n",
    "                s_text_dict[speaker_counted] = \"\"  # 喋る内容の準備\n",
    "\n",
    "            else: #前にも話していた時\n",
    "                speak_counter_dict[speaker] = speak_counter_dict[speaker] + 1 #会話数をプラス１\n",
    "                speaker_counted = speaker + \" \" + str(speak_counter_dict[speaker])\n",
    "                s_text_dict[speaker_counted] = \"\"\n",
    "\n",
    "        s_text_dict[speaker_counted] += value\n",
    "    return s_text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1960c605-cf4c-467f-9398-850bbfb05b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_dict = read_clova_txt('text/clova_test.txt')\n",
    "dictional = read_sloos_csv('text/sloos_sample.csv')\n",
    "\n",
    "#グーグルの文字起こしデータ\n",
    "with open(\"text/speech_to_text.pkl\", \"rb\") as f:\n",
    "    response1 = pickle.load(f)\n",
    "result = response1.results[-1]\n",
    "words_info = result.alternatives[0].words\n",
    "\n",
    "\n",
    "###########################        文脈による分割      #####################  ->  context_dict { key_word : paragraph}\n",
    "all_text = ''\n",
    "for key, value in text_dict.items():\n",
    "    all_text += value\n",
    "\n",
    "#全てのテキストを綺麗にする\n",
    "p_all_text = text_clenging(all_text)\n",
    "#テキストを分割\n",
    "divide_list = divide_text2(p_all_text)\n",
    "#分割したテキストを要約\n",
    "new_list = []\n",
    "for text in divide_list:\n",
    "    new_list.append(summarize(text))\n",
    "divide_list = new_list\n",
    "#短すぎる文章を削除\n",
    "for text in divide_list:\n",
    "    if len(re.findall('。', text)) == 1:\n",
    "        divide_list.remove(text)\n",
    "#キーワードをくっつける\n",
    "context_dict = {}\n",
    "for sentence in divide_list:\n",
    "    key_word = get_key_word2(sentence)\n",
    "    #キーワードを整理\n",
    "    p_key_word = keyword_clenging(key_word)\n",
    "    context_dict[p_key_word] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a8c399b-225c-4176-a0cd-1ae794f45e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################    発話者による分割      ##########################    ->  {参加者 : {キーワード : 要約内容}}\n",
    "\n",
    "#テキストクリーニングと相槌削除\n",
    "p_text_dict = {}\n",
    "for key, value in text_dict.items():\n",
    "    value = text_clenging(value)\n",
    "    #value = preprocessor(value)\n",
    "    #value = summarize(value, 1)\n",
    "        \n",
    "    if len(value) > 100:\n",
    "        p_text_dict[key] = value \n",
    "        \n",
    "p_text_dict = delete_aizuti(p_text_dict)\n",
    "s_text_dict = text_merge(p_text_dict)\n",
    "\n",
    "#{参加者 1 0: キーワード}\n",
    "#{参加者 : {キーワード : 要約内容}}\n",
    "#key_text_dict = key_words(p_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483265f4-267f-467b-9394-d3db38f85376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wikipediaリンク用の単語を抽出\n",
    "words = []\n",
    "for key, value in s_text_dict.items():\n",
    "    words.extend(severe_extract(value))\n",
    "#重複を削除\n",
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e39b9888-dd11-4746-bb7a-92de4c166215",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wikipediaに存在するか確認\n",
    "checked_words = []\n",
    "for word in words:\n",
    "    \n",
    "    request_url = 'https://ja.wikipedia.org/api/rest_v1/page/summary/' + urllib.parse.quote(word)\n",
    "    req = Request(request_url)\n",
    "    try:\n",
    "        with urlopen(req) as res:\n",
    "            res_json = res.read()\n",
    "    except HTTPError as e:\n",
    "        continue\n",
    "    except URLError as e:\n",
    "        continue\n",
    "    else:\n",
    "        wiki = json.loads(res_json.decode('utf-8'))\n",
    "        if (wiki['type'] == 'disambiguation') or (len(wiki['extract']) < 7):\n",
    "            continue\n",
    "\n",
    "        checked_words.append(word)\n",
    "    time.sleep(0.01)\n",
    "\n",
    "#リンクが貼れるように置換するための辞書を作成\n",
    "checked_dict = {}\n",
    "for word in checked_words:\n",
    "    checked_dict[word] = \"<span class=\\\"wmf-wp-with-preview\\\" data-wp-title=\\\"\"+ word + \"\\\" data-wikipedia-preview>\" + word + \"</span>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b2f1f3e1-1c01-44d2-be70-2089b4b09314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_text_dict = {}\n",
    "ss_text_dict = {}\n",
    "for key, value in s_text_dict.items():\n",
    "    count = len(value.split('。'))\n",
    "\n",
    "    value = summarize(value, round(count/5))#20%のこし\n",
    "    \n",
    "    confirm_value = \"\"\n",
    "    if len(value) > 500:\n",
    "        count = len(value.split('。'))\n",
    "        confirm_value = summarize(value, round(count/5))#20%のこし\n",
    "    \n",
    "    \n",
    "    key_word = get_key_word2(value)\n",
    "    p_key_word = keyword_clenging(key_word)\n",
    "    \n",
    "    \n",
    "    #辞書をもとにhtml用に書き換え\n",
    "    for word, read in checked_dict.items():\n",
    "        value = value.replace(word, read)\n",
    "        confirm_value = confirm_value.replace(word, read)\n",
    "    \n",
    "    #2回replaceされる単語を治す\n",
    "    value = re.sub(r'<span class=\"wmf-wp-with-preview\" data-wp-title=\"<span class=\"wmf-wp-with-preview\" data-wp-title=\"([^\"]+)\" data-wikipedia-preview>[^>]+>([^\"]+)[^>]+>[^>]+>[^<]+[^>]+.', \\\n",
    "       r'<span class=\"wmf-wp-with-preview\" data-wp-title=\"\\1\\2\" data-wikipedia-preview>\\1' , value)\n",
    "    confirm_value = re.sub(r'<span class=\"wmf-wp-with-preview\" data-wp-title=\"<span class=\"wmf-wp-with-preview\" data-wp-title=\"([^\"]+)\" data-wikipedia-preview>[^>]+>([^\"]+)[^>]+>[^>]+>[^<]+[^>]+.', \\\n",
    "       r'<span class=\"wmf-wp-with-preview\" data-wp-title=\"\\1\\2\" data-wikipedia-preview>\\1' , confirm_value)\n",
    "    \n",
    "    if confirm_value == '':\n",
    "        div = '''\n",
    "        <div class=\"confirm_value\">\n",
    "            <p>{value}</p>\n",
    "        </div>'''.format(value=value)\n",
    "    else:\n",
    "        div = '''\n",
    "        <div class=\"confirm_value\" onclick=\"obj=document.getElementById('{key}').style; obj.display=(obj.display=='none')?'block':'none';\">\n",
    "            <p>{confirm_value}</p>\n",
    "            <p class='see'>Click to see more</p>\n",
    "        </div>\n",
    "        <div class=\"more\" id=\"{key}\" style=\"display:none;clear:both;\">\n",
    "            <p>{value}</p>\n",
    "        </div>'''.format(key=key, confirm_value=confirm_value, value=value)\n",
    "    \n",
    "    ss_text_dict[key] = (p_key_word, div)\n",
    "\n",
    "body=\"\"\n",
    "for key, value in ss_text_dict.items():  ##発話者による分割\n",
    "    body = body + '''\n",
    "    <h2>{key}</h2>\n",
    "    <h3>{keyword}</h3>{div}'''.format(key=key, keyword=value[0], div=value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1eb36711-d717-4222-991b-8c8878648691",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#body=\"\"\n",
    "#for key, value in context_dict.items():  ##文脈による分割\n",
    "#for key, value in ss_text_dict.items():  ##発話者による分割\n",
    "#for key, value in key_text_dict.items():\n",
    "#    body = body + \"<h2>{key}</h2><p>{value}</p>\".format(key=key, value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0fc29741-6248-4882-81af-c954f2e468c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt_now = datetime.datetime.now()\n",
    "date=dt_now.strftime('%Y年%m月%d日') + \"の議事録\"\n",
    "\n",
    "str1 = '''<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{title}</title>\n",
    "    <link rel=\"stylesheet\" href=\"static/style.css\">\n",
    "    <style>\n",
    "        [data-wikipedia-preview] {{ background-color: blue; }}\n",
    "    </style>\n",
    "</head>\n",
    "    <body>\n",
    "        <h1>{date}</h1>\n",
    "        <h1>画像アップロード</h1>\n",
    "        <form action=\"upload.php\" method=\"post\" enctype=\"multipart/form-data\" class=\"form-img\">\n",
    "            <div id=\"drop-zone\" style=\"border: 1px solid; padding: 30px; border-color: white;\">\n",
    "                <p>ファイルをドラッグ＆ドロップもしくは</p>\n",
    "                <input type=\"file\" name=\"file\" id=\"file-input\">\n",
    "            </div>\n",
    "            <h2>プレビュー</h2>\n",
    "            <div id=\"preview\"></div>\n",
    "            <h2>アップロードした画像</h2>\n",
    "            <div id=\"uploaded\"></div>\n",
    "            <input type=\"submit\" style=\"margin-top: 50px\">\n",
    "        </form>\n",
    "        {body}\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js\"></script>\n",
    "        <script src=\"static/wikipedia-preview.development.js\"></script>\n",
    "        <script src=\"static/file.js\"></script>\n",
    "        <script type=\"text/javascript\">wikipediaPreview.init({{lang: 'ja'}});</script>\n",
    "    </body>\n",
    "</html>'''.format(title='議事録', date=date, body=body) \n",
    "\n",
    "write1(os.path.abspath(\"../output/public/gijiroku\")+'/gijiroku'+ datetime.datetime.now().strftime('(%Y.%m.%d)') + '.html', str1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956faff-a975-48f7-bc1f-b242638efd63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
