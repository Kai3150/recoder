{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d1dc8206-16c6-4cd2-a420-bbc934ec6e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# scikit-learnのTF-IDFライブラリをインポート\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import MeCab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pkg_resources, imp\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import gensim\n",
    "\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4ab16c9d-089b-41db-a536-d9ba57ba2320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_clova_txt(clova_txt_path):\n",
    "    \n",
    "    text_dict = {}\n",
    "    speaker = \"\"\n",
    "    speaker_counted = \"\" #辞書のキー\n",
    "    speak_counter_dict = {}\n",
    "\n",
    "    \n",
    "    with open(clova_txt_path) as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            line = line.rstrip()  # 読み込んだ行の末尾には改行文字があるので削除\n",
    "            if re.search(r'^参加者', line):\n",
    "                if speaker != line:  #話者が変わるとき\n",
    "                    speaker = line   #話者を更新\n",
    "\n",
    "                    #if speaker_counted not in text_dict.keys():\n",
    "                    if speaker not in speak_counter_dict.keys(): #初めて喋る人\n",
    "                        speak_counter_dict[speaker] = 0\n",
    "                        speaker_counted = speaker + str(speak_counter_dict[speaker])\n",
    "                        text_dict[speaker_counted] = \"\"  # 喋る内容の準備\n",
    "\n",
    "                    else: #前にも話していた時\n",
    "                        speak_counter_dict[speaker] = speak_counter_dict[speaker] + 1 #会話数をプラス１\n",
    "                        speaker_counted = speaker + str(speak_counter_dict[speaker])\n",
    "                        text_dict[speaker_counted] = \"\"\n",
    "\n",
    "            else:\n",
    "                text_dict[speaker_counted] += line\n",
    "    return text_dict\n",
    "\n",
    "def read_sloos_csv(sloos_csv_path):\n",
    "\n",
    "    df = pd.read_csv(sloos_csv_path)\n",
    "\n",
    "    text_dict = {}\n",
    "    speaker = \"\"\n",
    "    speaker_counted = \"\" #辞書のキー\n",
    "    speak_counter_dict = {}\n",
    "\n",
    "\n",
    "    for row in df.iterrows():\n",
    "\n",
    "        if speaker != row[1]['speaker']:  #話者が変わるとき\n",
    "            speaker = row[1]['speaker']   #話者を更新\n",
    "\n",
    "            #if speaker_counted not in text_dict.keys():\n",
    "            if speaker not in speak_counter_dict.keys(): #初めて喋る人\n",
    "                speak_counter_dict[speaker] = 0\n",
    "                speaker_counted = str(speaker) + str(speak_counter_dict[speaker])\n",
    "                text_dict[speaker_counted] = \"\"  # 喋る内容の準備\n",
    "\n",
    "            else: #前にも話していた時\n",
    "                speak_counter_dict[speaker] = speak_counter_dict[speaker] + 1 #会話数をプラス１\n",
    "                speaker_counted = str(speaker) + str(speak_counter_dict[speaker])\n",
    "                text_dict[speaker_counted] = \"\"\n",
    "\n",
    "        text_dict[speaker_counted] += row[1]['message'] + '。'\n",
    "\n",
    "    return text_dict\n",
    "\n",
    "def summarize(text: str, count=10) -> str:\n",
    "    LANGUAGE = \"japanese\"  # 言語指定\n",
    "    SENTENCES_COUNT = count  # 要約文数\n",
    "\n",
    "\n",
    "    # parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    sentences = \"\"\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sentences = sentences + sentence.__str__()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub(' ', '', text) ##空白削除\n",
    "   \n",
    "    text = text.replace('です', 'です。').replace('ます','ます。').replace('でした','でした。').replace('ません','ません。')##ますの後には必ず「。」\n",
    "    text = re.sub(r'[、。](ただ|でも|いや|だが|しかし|対して|一方で|あるいは|けれども|けども|けど|が)[、。]', '#BUT', text) ##逆説->#BUT\n",
    "    text = re.sub(r'[、。](たとえば|例えば)[、。]', '#EXAMPLE', text) ##具体例->#EXAMPLE\n",
    "    text = re.sub(r'[、。](だって)[、。]', '#BECAUSE_REASON', text) ##理由1->#BECAUSE_REASON\n",
    "    text = re.sub(r'[、。](だから|なので|従って)[、。]', '#REASON_BECAUSE', text) ##理由2->#REASON_BECAUSE\n",
    "    text = re.sub(r'[、。](つまり|まとめると|ですから)[、。]', '#INANUT', text) ##まとめor重要->#INANUT\n",
    "    text = re.sub(r'[、。](加えて|それと|そして|それから)[、。]', '#AND', text) ##加えて->#AND\n",
    "\n",
    "\n",
    "    text = re.sub(r'[、].{0,5}[、。]', '、', text) ##削除\n",
    "    text = re.sub(r'[。].{0,5}[、。]', '。', text) ##削除\n",
    "    text = re.sub(r'(えー)', '', text) ##削除\n",
    "\n",
    "    text = re.sub(r'(え[、。])', '、', text) ##誤字訂正\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'ます、', 'ます。', text) ##ますの後には必ず「。」\n",
    "    #re.findall(r'[、。].{6}[、。]', text)\n",
    "    \n",
    "    #print(text)\n",
    "    return text\n",
    "\n",
    "def write1(file, str1): \n",
    "    with open(file, 'w', encoding='utf-8') as f1: \n",
    "        f1.write(str1) \n",
    "\n",
    "        \n",
    "        \n",
    "def convert(text):\n",
    "\n",
    "    # ノイズ削除\n",
    "    text = re.sub(r'《.+?》', '', text)\n",
    "    text = re.sub(r'［＃.+?］', '', text)\n",
    "    text = re.sub(r'｜', '', text)\n",
    "    text = re.sub(r'\\r\\n', '', text)\n",
    "    text = re.sub(r'\\u3000', '', text)  \n",
    "    text = re.sub(r'「', '', text) \n",
    "    text = re.sub(r'」', '', text)\n",
    "    text = re.sub(r'、', '', text)\n",
    "    text = re.sub(r'。', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Taggerオブジェクトを生成\n",
    "tokenizer = MeCab.Tagger(\"-Ochasen\")\n",
    "tokenizer.parse(\"\")\n",
    "\n",
    "def extract(text):\n",
    "    words = []\n",
    "\n",
    "    # 単語の特徴リストを生成\n",
    "    node = tokenizer.parseToNode(text)\n",
    "\n",
    "    while node:\n",
    "        # 品詞情報(node.feature)が名詞ならば\n",
    "        if node.feature.split(\",\")[0] == u\"名詞\":\n",
    "            if re.fullmatch(r'[\\u3040-\\u309F]+', node.surface)== None: #ひらがなだけはパス\n",
    "                # 単語(node.surface)をwordsに追加\n",
    "                words.append(node.surface)\n",
    "        node = node.next\n",
    "\n",
    "    # 半角スペース区切りで文字列を結合\n",
    "    text_result = ' '.join(words)\n",
    "    return text_result\n",
    "\n",
    "def key_words(p_text_dict):\n",
    "    docs = []\n",
    "\n",
    "    for key, value in p_text_dict.items():\n",
    "        text = convert(value)\n",
    "        text = extract(text)\n",
    "        docs.append(text)\n",
    "        \n",
    "    # モデルを生成\n",
    "    vectorizer = TfidfVectorizer(smooth_idf=False)\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # データフレームに表現\n",
    "    values = X.toarray()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    df = pd.DataFrame(values, columns = feature_names, index=p_text_dict.keys())\n",
    "\n",
    "    top10_dict = {}\n",
    "    for key in p_text_dict.keys():\n",
    "        top10_dict[key] = df.T[key].sort_values(ascending=False).head(10).keys()\n",
    "    \n",
    "    return top10_dict\n",
    "\n",
    "def get_key_word(text):\n",
    "    docs = []\n",
    "\n",
    "    text = convert(text)\n",
    "    text = extract(text)\n",
    "\n",
    "    # モデルを生成\n",
    "    vectorizer = TfidfVectorizer(smooth_idf=False)\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # データフレームに表現\n",
    "    values = X.toarray()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "\n",
    "#文章を文脈で分割\n",
    "\n",
    "#わかち書き関数\n",
    "def wakachi(text):\n",
    "    from janome.tokenizer import Tokenizer\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(text)\n",
    "    docs=[]\n",
    "    for token in tokens:\n",
    "        docs.append(token.surface)\n",
    "    return docs\n",
    " \n",
    "#文書ベクトル化関数\n",
    "def vecs_array(documents):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "    docs = np.array(documents)\n",
    "    vectorizer = TfidfVectorizer(analyzer=wakachi,binary=True,use_idf=False)\n",
    "    vecs = vectorizer.fit_transform(docs)\n",
    "    return vecs.toarray()\n",
    "\n",
    "def divide_text(text: str) -> list:\n",
    "    \n",
    "    #三文ごとに分割\n",
    "    sample_slice = re.findall(\"[^。]+。?\", text)\n",
    "\n",
    "    i = 0\n",
    "    new_slice = []\n",
    "    temp_slice = []\n",
    "    for sentence in sample_slice:\n",
    "        temp_slice.append(sentence)\n",
    "\n",
    "        if i % 3 == 2:\n",
    "            new_slice.append(''.join(temp_slice))\n",
    "            temp_slice = []\n",
    "        i += 1\n",
    "    \n",
    "    docs = new_slice\n",
    "\n",
    "    #類似度行列作成\n",
    "    cs_array = np.round(cosine_similarity(vecs_array(docs), vecs_array(docs)),3)\n",
    "\n",
    "    xy=np.where(cs_array < 0.3)\n",
    "\n",
    "    x = xy[0][abs(xy[0] - xy[1]) == 1]\n",
    "    y = xy[1][abs(xy[0] - xy[1]) == 1]\n",
    "\n",
    "    diffs = pd.DataFrame([x, y])\n",
    "    t = diffs.T\n",
    "    t = t[t[0] > t[1]]\n",
    "    \n",
    "    #<h3>{key_words}</h3>\n",
    "    #<p>{content}</p>\n",
    "\n",
    "    #content = \"\"\" <h4>{speaker}</h4>  \n",
    "    #               <p>{naiyou}</p>           \"\"\"\n",
    "\n",
    "    #context_dict { key_words: contents}\n",
    "\n",
    "    for _, item in t.iterrows():\n",
    "        new_slice[item[0]] += '##DIV##'  \n",
    "\n",
    "    new_text = ''.join(new_slice)\n",
    "    divided_list = new_text.split('##DIV##')\n",
    "    \n",
    "    #add_key_words(new_slice)\n",
    "    #return context_dict\n",
    "    return divided_list\n",
    "def divide_text2(text: str) -> list:\n",
    "    slice_list = re.findall(\"([^。]+。)\", text)\n",
    "    while len(slice_list) > 30:\n",
    "        max_ratio = 0\n",
    "        divide_list = []\n",
    "        sames = []\n",
    "\n",
    "        for i in range(len(slice_list)-1):\n",
    "\n",
    "            str1 = slice_list[i]\n",
    "            str2 = slice_list[i+1]\n",
    "            \n",
    "\n",
    "            s = SequenceMatcher(None, str1, str2)\n",
    "\n",
    "            if s.ratio() > max_ratio:\n",
    "                max_ratio = s.ratio()\n",
    "                max_i = i\n",
    "                divide_list = [str1, str2]\n",
    "\n",
    "        slice_list[max_i] = slice_list[max_i] + slice_list.pop(max_i + 1)\n",
    "    return slice_list\n",
    "\n",
    "def add_key_words(new_slice):\n",
    "    context_list = []\n",
    "    key_word = 0\n",
    "    contents = ''\n",
    "    context_dict = {}\n",
    "    \n",
    "\n",
    "    for sentence3 in new_slice:\n",
    "        if '##DIV##' in sentence3:\n",
    "            sentence3 = sentence3.replace('##DIV##', '')\n",
    "            contents = ''\n",
    "            contents += sentence3\n",
    "\n",
    "            ##キーワードの抽出するならここ\n",
    "            key_word = key_word + 1\n",
    "\n",
    "            #context_list.append(contents)\n",
    "            context_dict[key_word] = contents\n",
    "        else:\n",
    "            contents += sentence3\n",
    "    return context_dict\n",
    "\n",
    "\n",
    "imp.reload(pkg_resources)\n",
    "# モデルのロード\n",
    "nlp = spacy.load(\"ja_core_news_md\")\n",
    "\n",
    "def get_key_word2(input_sentence):\n",
    "    # 解析対象のテキストa\n",
    "    input_text = input_sentence\n",
    "\n",
    "    input_text = re.sub(r'[#A-Z_]+', '', input_text)\n",
    "    # モデルに解析対象のテキストを渡す\n",
    "    doc = nlp(input_text)\n",
    "    # 固有表現を抽出\n",
    "    #for ent in doc.ents: \n",
    "        #print(ent.text, ent.label_, ent.start_char, ent.end_char)\n",
    "\n",
    "    # out --> \n",
    "    ## 2018年 DATE 0 5\n",
    "    ## 8月 DATE 6 8\n",
    "    ## フランス GPE 12 16\n",
    "    ## ルーヴル美術館 ORG 24 31\n",
    "    key_words = []\n",
    "    for ent in doc.ents: \n",
    "        if ent.label_ in ['ORG', 'PERSON']:\n",
    "            key_words.append(ent.text)\n",
    "    key_words = list(set(key_words))#重複要素を削除\n",
    "    key_str = '、'.join(key_words)\n",
    "    return key_str\n",
    "\n",
    "\n",
    "def clean_text_api(text):\n",
    "    API=\"https://api.a3rt.recruit.co.jp/proofreading/v2/typo\"\n",
    "    KEY=\"DZZps9cHfJGAxGlvhqkYh0xHlIk8igKu\"\n",
    " \n",
    "    cleaned_text = ''\n",
    "    text_list = re.findall(\"[^。]+。?\", text)\n",
    "    for sentence in text_list:\n",
    "        quoted_text = sentence\n",
    "        values = {\n",
    "        'apikey': KEY,\n",
    "        'sentence':quoted_text,\n",
    "        'sensitivity':\"low\",\n",
    "        }\n",
    "\n",
    "        # パラメータをURLエンコードする\n",
    "        params = urllib.parse.urlencode(values)\n",
    "        # リクエスト用のURLを生成\n",
    "        url = API + \"?\" + params\n",
    "\n",
    "        #リクエストを投げて結果を取得\n",
    "        r = requests.get(url)\n",
    "        #辞書型に変換\n",
    "        data = json.loads(r.text)\n",
    "\n",
    "        if 'alerts' in data:\n",
    "\n",
    "            for alert in data['alerts']:\n",
    "                miss_word = alert['word']\n",
    "                suggested_word = alert['suggestions'][0]\n",
    "                quoted_text = quoted_text.replace(miss_word, suggested_word)\n",
    "        cleaned_text = cleaned_text + quoted_text\n",
    "    return cleaned_text\n",
    "def text_clenging(text:str) -> str:\n",
    "    \n",
    "    text = re.sub(' ', '、', text) ##空白削除\n",
    "   \n",
    "    text = text.replace('です', 'です。').replace('ます','ます。').replace('でした','でした。').replace('ません','ません。').replace('さい','さい。')##ますの後には必ず「。」\n",
    "    text = text.replace('っていうこと', 'こと').replace('っていう', 'という').replace('ていう', 'という').replace('かなと', 'かと')##ますの後には必ず「。」\n",
    "    \n",
    "    text = re.sub(r'(えー|えーと|えっと|そうですね|まあ|じゃあ|なんか|ちょっと|あの|ということで|っていうの|んじゃないか|一応|とりあえず)', '', text) ##削除\n",
    "    text = re.sub(r'ま([^\\u3040-\\u309F])', r'\\1', text) ##削除\n",
    "    text = re.sub(r'という([、。])', r'\\1', text) ##削除\n",
    "    text = re.sub(r'(.)(.)(.)\\1\\2\\3', r'\\1\\2\\3', text)#繰り返し文字\n",
    "    text = re.sub(r'([\\u3400-\\u9FFF\\uF900-\\uFAFF]|[\\uD840-\\uD87F][\\uDC00-\\uDFFF])([\\u3400-\\u9FFF\\uF900-\\uFAFF]|[\\uD840-\\uD87F][\\uDC00-\\uDFFF])\\1\\2', r'\\1\\2', text)\n",
    "    \n",
    "    while re.search(r'([ねえま][、。]|[、。].{0,2}[、。])', text):\n",
    "        text = re.sub(r'([ねえま]、)', '、', text) ##誤字 語感　訂正\n",
    "        text = re.sub(r'([ねえま]。)', '。', text) ##誤字　語感　訂正\n",
    "        \n",
    "        text = re.sub(r'[、].{0,2}[、。]', '、', text) ##削除\n",
    "        text = re.sub(r'[。].{0,2}[、。]', '。', text) ##削除\n",
    "        \n",
    "\n",
    "    return text\n",
    "\n",
    "def keyword_clenging(key_str: str)->str:\n",
    "    key_str = re.sub(r'(宮崎ゼミ|宮崎|ゼミ)', '', key_str)\n",
    "    key_str = re.sub(r'(、、)', '、', key_str)\n",
    "    key_str = re.sub(r'^、', '', key_str)\n",
    "    return key_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2b6b743b-3010-4651-b7f5-7b3b9ab3f7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import collections\n",
    "#l = re.findall(r'(.)\\1\\1', p_all_text)\n",
    "#collections.Counter(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1960c605-cf4c-467f-9398-850bbfb05b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_dict = read_clova_txt('clova_test.txt')\n",
    "dictional = read_sloos_csv('sloos_sample.csv')\n",
    "\n",
    "all_text = ''\n",
    "for key, value in text_dict.items():\n",
    "    all_text += value\n",
    "\n",
    "p_all_text = text_clenging(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ab2cee43-6c43-4449-8a47-b623eb6c73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#テキストを分割\n",
    "divide_list = divide_text2(p_all_text)\n",
    "\n",
    "#分割したテキストを要約\n",
    "new_list = []\n",
    "for text in divide_list:\n",
    "    new_list.append(summarize(text))\n",
    "divide_list = new_list\n",
    "\n",
    "#短すぎる文章を削除\n",
    "for text in divide_list:\n",
    "    if len(re.findall('。', text)) == 1:\n",
    "        divide_list.remove(text)\n",
    "\n",
    "#キーワードをくっつける\n",
    "context_dict = {}\n",
    "for sentence in divide_list:\n",
    "    \n",
    "    key_word = get_key_word2(sentence)\n",
    "    p_key_word = keyword_clenging(key_word)\n",
    "    context_dict[p_key_word] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1eb36711-d717-4222-991b-8c8878648691",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_now = datetime.datetime.now()\n",
    "date=dt_now.strftime('%Y年%m月%d日') + \"の議事録\"\n",
    "\n",
    "body=\"\"\n",
    "\n",
    "for key, value in context_dict.items():\n",
    "    body = body + \"<h2>{key}</h2><p>{value}</p>\".format(key=key, value=value)\n",
    "\n",
    "str1 = '''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{title}</title>\n",
    "    <link rel=\"stylesheet\" href=\"style.css\">\n",
    "</head>\n",
    "    <body>\n",
    "        <h1>{date}</h1>\n",
    "        {body} \n",
    "    </body>\n",
    "</html>\n",
    "'''.format(title='議事録', date=date, body=body) \n",
    "\n",
    "write1('sample.html', str1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d4d432b-17ec-46a5-b465-d5b1072a4f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_text_dict = {}\n",
    "for key in text_dict.keys():\n",
    "    value = preprocessor(text_dict[key])\n",
    "    #value = summarize(value, 1)\n",
    "    p_text_dict[key] = value\n",
    "\n",
    "\n",
    "dt_now = datetime.datetime.now()\n",
    "\n",
    "\n",
    "date=dt_now.strftime('%Y年%m月%d日') + \"の議事録\"\n",
    "\n",
    "body=\"\"\n",
    "\n",
    "for key, value in key_words(p_text_dict).items():\n",
    "    body = body + \"<h2>{key}</h2><p>{value}</p>\".format(key=key, value=value)\n",
    "\n",
    "str1 = '''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{title}</title>\n",
    "</head>\n",
    "    <body>\n",
    "        <h1>{date}</h1>\n",
    "        {body} \n",
    "    </body>\n",
    "</html>\n",
    "'''.format(title='議事録', date=date, body=body) \n",
    "\n",
    "write1('sample.html', str1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d808384-43ac-4697-bcb8-e97de7205dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
